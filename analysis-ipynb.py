{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Initialization Analysis\n",
    "\n",
    "This notebook analyzes and visualizes the results of our weight initialization experiments on ResNet-18 and DistilBERT models.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "\n",
    "# Define directories\n",
    "RESULTS_DIR = \"./results\"\n",
    "RESNET_METRICS_DIR = os.path.join(RESULTS_DIR, \"resnet\", \"metrics\")\n",
    "DISTILBERT_METRICS_DIR = os.path.join(RESULTS_DIR, \"distilbert\", \"metrics\")\n",
    "LOG_DIR = os.path.join(RESULTS_DIR, \"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_metrics(dir_path, model_type, init_methods):\n",
    "    \"\"\"Load metrics for all initialization methods.\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    for method in init_methods:\n",
    "        file_path = os.path.join(dir_path, f\"{model_type}_{method}_metrics.json\")\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r') as f:\n",
    "                metrics[method] = json.load(f)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# List of initialization methods\n",
    "init_methods = [\"xavier\", \"he\", \"lecun\", \"random\"]\n",
    "\n",
    "# Load metrics\n",
    "resnet_metrics = load_metrics(RESNET_METRICS_DIR, \"resnet\", init_methods)\n",
    "distilbert_metrics = load_metrics(DISTILBERT_METRICS_DIR, \"distilbert\", init_methods)\n",
    "\n",
    "# Load summary if available\n",
    "summary_path = os.path.join(LOG_DIR, \"overall_summary.json\")\n",
    "if os.path.exists(summary_path):\n",
    "    with open(summary_path, 'r') as f:\n",
    "        overall_summary = json.load(f)\n",
    "    print(\"Overall summary loaded.\")\n",
    "else:\n",
    "    overall_summary = None\n",
    "    print(\"Overall summary not found. Run evaluations first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet-18 Results\n",
    "\n",
    "### Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_metric_curves(metrics_dict, metric_key, title, xlabel=\"Epoch\", ylabel=\"Value\"):\n",
    "    \"\"\"Plot curves for a specific metric across initialization methods.\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "    line_styles = ['-', '--', '-.', ':']\n",
    "    \n",
    "    for i, (method, metrics) in enumerate(metrics_dict.items()):\n",
    "        if metric_key in metrics:\n",
    "            values = metrics[metric_key]\n",
    "            epochs = range(1, len(values) + 1)\n",
    "            plt.plot(epochs, values, \n",
    "                     label=method.capitalize(), \n",
    "                     linewidth=3, \n",
    "                     color=colors[i % len(colors)],\n",
    "                     linestyle=line_styles[i % len(line_styles)],\n",
    "                     marker='o')\n",
    "    \n",
    "    plt.title(title, fontsize=18)\n",
    "    plt.xlabel(xlabel, fontsize=14)\n",
    "    plt.ylabel(ylabel, fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Plot ResNet validation accuracy\n",
    "if resnet_metrics:\n",
    "    plt_resnet_acc = plot_metric_curves(\n",
    "        resnet_metrics, \n",
    "        \"val_accuracies\", \n",
    "        \"ResNet-18 Validation Accuracy by Initialization Method\",\n",
    "        ylabel=\"Accuracy (%)\"\n",
    "    )\n",
    "    plt_resnet_acc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_bar_comparison(metrics_dict, metric_key, title, ylabel=\"Value\", sort=True):\n",
    "    \"\"\"Create a bar plot comparing a metric across initialization methods.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    data = {}\n",
    "    for method, metrics in metrics_dict.items():\n",
    "        if metric_key in metrics:\n",
    "            data[method.capitalize()] = metrics[metric_key]\n",
    "    \n",
    "    if sort:\n",
    "        data = dict(sorted(data.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    methods = list(data.keys())\n",
    "    values = list(data.values())\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "    bars = plt.bar(methods, values, color=colors[:len(methods)])\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.2f}', ha='center', fontsize=12)\n",
    "    \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.ylabel(ylabel, fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Plot convergence comparison\n",
    "if resnet_metrics:\n",
    "    # Extract convergence epochs\n",
    "    convergence_data = {}\n",
    "    for method, metrics in resnet_metrics.items():\n",
    "        if 'convergence_epoch' in metrics:\n",
    "            convergence_data[method] = metrics['convergence_epoch']\n",
    "    \n",
    "    if convergence_data:\n",
    "        plt_convergence = plot_bar_comparison(\n",
    "            {m: convergence_data[m] for m in convergence_data},\n",
    "            None,  # Already extracted the data\n",
    "            \"ResNet-18 Convergence Speed (Lower is Better)\",\n",
    "            ylabel=\"Epochs to Converge\",\n",
    "            sort=False  # Lower is better, so don't sort descending\n",
    "        )\n",
    "        plt_convergence.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loss Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot ResNet training loss\n",
    "if resnet_metrics:\n",
    "    plt_resnet_loss = plot_metric_curves(\n",
    "        resnet_metrics, \n",
    "        \"train_losses\", \n",
    "        \"ResNet-18 Training Loss by Initialization Method\",\n",
    "        ylabel=\"Loss\"\n",
    "    )\n",
    "    plt_resnet_loss.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Norm Analysis\n",
    "\n",
    "Gradient norms provide insight into training stability and potential for exploding/vanishing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot ResNet gradient norms\n",
    "if resnet_metrics:\n",
    "    plt_resnet_grad = plot_metric_curves(\n",
    "        resnet_metrics, \n",
    "        \"grad_norms\", \n",
    "        \"ResNet-18 Gradient Norm by Initialization Method\",\n",
    "        ylabel=\"Gradient L2 Norm\"\n",
    "    )\n",
    "    plt_resnet_grad.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract final accuracies\n",
    "if resnet_metrics:\n",
    "    final_acc_data = {}\n",
    "    for method, metrics in resnet_metrics.items():\n",
    "        if 'final_accuracy' in metrics:\n",
    "            final_acc_data[method] = metrics['final_accuracy']\n",
    "    \n",
    "    if final_acc_data:\n",
    "        plt_final_acc = plot_bar_comparison(\n",
    "            {m: final_acc_data[m] for m in final_acc_data},\n",
    "            None,  # Already extracted the data\n",
    "            \"ResNet-18 Final Accuracy by Initialization Method\",\n",
    "            ylabel=\"Accuracy (%)\"\n",
    "        )\n",
    "        plt_final_acc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT Results\n",
    "\n",
    "### Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot DistilBERT validation accuracy\n",
    "if distilbert_metrics:\n",
    "    plt_distilbert_acc = plot_metric_curves(\n",
    "        distilbert_metrics, \n",
    "        \"val_accuracies\", \n",
    "        \"DistilBERT Validation Accuracy by Initialization Method\",\n",
    "        ylabel=\"Accuracy (%)\"\n",
    "    )\n",
    "    plt_distilbert_acc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Stability Analysis\n",
    "\n",
    "For transformer models like DistilBERT, gradient stability is particularly important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot DistilBERT gradient norms\n",
    "if distilbert_metrics:\n",
    "    plt_distilbert_grad = plot_metric_curves(\n",
    "        distilbert_metrics, \n",
    "        \"grad_norms\", \n",
    "        \"DistilBERT Gradient Norm by Initialization Method\",\n",
    "        ylabel=\"Gradient L2 Norm\"\n",
    "    )\n",
    "    plt_distilbert_grad.show()\n",
    "    \n",
    "    # Calculate and plot gradient explosion reduction percentage\n",
    "    if 'random' in distilbert_metrics:\n",
    "        ref_grad_norm = max(distilbert_metrics['random'].get('grad_norms', [0]))\n",
    "        \n",
    "        grad_reductions = {}\n",
    "        for method, metrics in distilbert_metrics.items():\n",
    "            if method != 'random' and 'grad_norms' in metrics:\n",
    "                method_grad_norm = max(metrics['grad_norms'])\n",
    "                reduction_pct = (ref_grad_norm - method_grad_norm) / ref_grad_norm * 100\n",
    "                grad_reductions[method] = reduction_pct\n",
    "        \n",
    "        if grad_reductions:\n",
    "            plt_grad_reduction = plot_bar_comparison(\n",
    "                {m: grad_reductions[m] for m in grad_reductions},\n",
    "                None,  # Already extracted the data\n",
    "                \"Gradient Explosion Reduction vs. Random Initialization\",\n",
    "                ylabel=\"Reduction Percentage (%)\"\n",
    "            )\n",
    "            plt_grad_reduction.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract final accuracies for DistilBERT\n",
    "if distilbert_metrics:\n",
    "    final_acc_data = {}\n",
    "    for method, metrics in distilbert_metrics.items():\n",
    "        if 'final_accuracy' in metrics:\n",
    "            final_acc_data[method] = metrics['final_accuracy']\n",
    "    \n",
    "    if final_acc_data:\n",
    "        plt_final_acc = plot_bar_comparison(\n",
    "            {m: final_acc_data[m] for m in final_acc_data},\n",
    "            None,  # Already extracted the data\n",
    "            \"DistilBERT Final Accuracy by Initialization Method\",\n",
    "            ylabel=\"Accuracy (%)\"\n",
    "        )\n",
    "        plt_final_acc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis\n",
    "\n",
    "Let's compare how different initialization methods perform across both model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_comparative_heatmap(resnet_data, distilbert_data, metric_name, title, vmin=None, vmax=None, cmap=\"YlGnBu\"):\n",
    "    \"\"\"Create a heatmap to compare performance across models and initialization methods.\"\"\"\n",
    "    # Create a DataFrame for the heatmap\n",
    "    data = {\n",
    "        'ResNet-18': resnet_data,\n",
    "        'DistilBERT': distilbert_data\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = sns.heatmap(df, annot=True, fmt=\".2f\", cmap=cmap, vmin=vmin, vmax=vmax,\n",
    "                     cbar_kws={'label': metric_name})\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Compare final accuracy across models\n",
    "if resnet_metrics and distilbert_metrics:\n",
    "    resnet_acc = {}\n",
    "    distilbert_acc = {}\n",
    "    \n",
    "    for method in init_methods:\n",
    "        if method in resnet_metrics and 'final_accuracy' in resnet_metrics[method]:\n",
    "            resnet_acc[method.capitalize()] = resnet_metrics[method]['final_accuracy']\n",
    "        \n",
    "        if method in distilbert_metrics and 'final_accuracy' in distilbert_metrics[method]:\n",
    "            distilbert_acc[method.capitalize()] = distilbert_metrics[method]['final_accuracy']\n",
    "    \n",
    "    if resnet_acc and distilbert_acc:\n",
    "        plt_comp_acc = plot_comparative_heatmap(\n",
    "            resnet_acc,\n",
    "            distilbert_acc,\n",
    "            \"Accuracy (%)\",\n",
    "            \"Final Accuracy Comparison Across Models and Initialization Methods\"\n",
    "        )\n",
    "        plt_comp_acc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare gradient stability across models\n",
    "if resnet_metrics and distilbert_metrics:\n",
    "    resnet_grad = {}\n",
    "    distilbert_grad = {}\n",
    "    \n",
    "    for method in init_methods:\n",
    "        if method in resnet_metrics and 'grad_norms' in resnet_metrics[method]:\n",
    "            resnet_grad[method.capitalize()] = max(resnet_metrics[method]['grad_norms'])\n",
    "        \n",
    "        if method in distilbert_metrics and 'grad_norms' in distilbert_metrics[method]:\n",
    "            distilbert_grad[method.capitalize()] = max(distilbert_metrics[method]['grad_norms'])\n",
    "    \n",
    "    if resnet_grad and distilbert_grad:\n",
    "        plt_comp_grad = plot_comparative_heatmap(\n",
    "            resnet_grad,\n",
    "            distilbert_grad,\n",
    "            \"Max Gradient Norm\",\n",
    "            \"Gradient Stability Comparison Across Models and Initialization Methods\",\n",
    "            cmap=\"YlOrRd_r\"  # Reversed colormap - lower values are better\n",
    "        )\n",
    "        plt_comp_grad.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings Summary\n",
    "\n",
    "Extracting and displaying the key findings from our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if overall_summary is not None:\n",
    "    print(\"\\n=== KEY FINDINGS ===\\n\")\n",
    "    \n",
    "    # ResNet findings\n",
    "    if 'resnet' in overall_summary:\n",
    "        resnet_summary = overall_summary['resnet']\n",
    "        print(f\"ResNet-18 Performance:\")\n",
    "        print(f\"- Best method: {resnet_summary['best_method']} initialization\")\n",
    "        print(f\"- Best accuracy: {resnet_summary['best_accuracy']:.2f}%\")\n",
    "        if 'fastest_method' in resnet_summary:\n",
    "            print(f\"- Fastest convergence: {resnet_summary['fastest_method']} in {resnet_summary['fastest_epochs']} epochs\")\n",
    "        print()\n",
    "    \n",
    "    # DistilBERT findings\n",
    "    if 'distilbert' in overall_summary:\n",
    "        distilbert_summary = overall_summary['distilbert']\n",
    "        print(f\"DistilBERT Performance:\")\n",
    "        print(f\"- Best method: {distilbert_summary['best_method']} initialization\")\n",
    "        print(f\"- Best accuracy: {distilbert_summary['best_accuracy']:.2f}%\")\n",
    "        if 'best_stability_method' in distilbert_summary:\n",
    "            print(f\"- Best stability: {distilbert_summary['best_stability_method']} initialization\")\n",
    "            print(f\"- Gradient explosion reduction: {distilbert_summary['gradient_reduction']:.2f}% vs. random\")\n",
    "        print()\n",
    "    \n",
    "    # Cross-architecture insights\n",
    "    if 'conclusions' in overall_summary:\n",
    "        conclusions = overall_summary['conclusions']\n",
    "        print(f\"Cross-Architecture Insights:\")\n",
    "        if 'best_cnn_init' in conclusions and 'best_transformer_init' in conclusions:\n",
    "            if conclusions['best_cnn_init'] == conclusions['best_transformer_init']:\n",
    "                print(f\"- {conclusions['best_cnn_init']} initialization performs best for both architectures\")\n",
    "            else:\n",
    "                print(f\"- Different optimal initializations: {conclusions['best_cnn_init']} for CNN vs. {conclusions['best_transformer_init']} for Transformer\")\n",
    "                \n",
    "        # Check for theoretical vs. practical findings\n",
    "        if 'distilbert' in overall_summary and 'final_accuracies' in overall_summary['distilbert']:\n",
    "            if 'lecun' in overall_summary['distilbert']['final_accuracies']:\n",
    "                lecun_acc = overall_summary['distilbert']['final_accuracies']['lecun']\n",
    "                if lecun_acc < overall_summary['distilbert']['best_accuracy']:\n",
    "                    print(f\"- LeCun initialization underperformed for transformers despite theoretical advantages\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"\\nOverall summary not available. Please run the evaluation first to generate summary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Visualization: Performance vs. Stability Scatter Plot\n",
    "\n",
    "Create a scatter plot to visualize the trade-off between performance (accuracy) and stability (gradient norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_perf_vs_stability(metrics_dict, model_name):\n",
    "    \"\"\"Create a scatter plot of performance vs. stability.\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    x_values = []  # Max gradient norms (lower is better)\n",
    "    y_values = []  # Final accuracies (higher is better)\n",
    "    labels = []\n",
    "    \n",
    "    for method, metrics in metrics_dict.items():\n",
    "        if 'grad_norms' in metrics and 'final_accuracy' in metrics:\n",
    "            x_values.append(max(metrics['grad_norms']))\n",
    "            y_values.append(metrics['final_accuracy'])\n",
    "            labels.append(method.capitalize())\n",
    "    \n",
    "    # Create scatter plot\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "    \n",
    "    for i, (x, y, label) in enumerate(zip(x_values, y_values, labels)):\n",
    "        plt.scatter(x, y, s=200, c=[colors[i % len(colors)]], label=label, alpha=0.7)\n",
    "        plt.text(x+0.1, y, label, fontsize=14)\n",
    "    \n",
    "    plt.xlabel('Max Gradient Norm (lower is better)', fontsize=14)\n",
    "    plt.ylabel('Final Accuracy % (higher is better)', fontsize=14)\n",
    "    plt.title(f'{model_name} Performance vs. Stability', fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    \n",
    "    # Add quadrant labels to interpret the chart\n",
    "    x_mid = np.mean(x_values)\n",
    "    y_mid = np.mean(y_values)\n",
    "    \n",
    "    plt.axhline(y=y_mid, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.axvline(x=x_mid, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.text(min(x_values), max(y_values), 'Ideal: Stable & Accurate', \n",
    "             fontsize=10, ha='left', va='top', bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(max(x_values), max(y_values), 'Accurate but Unstable', \n",
    "             fontsize=10, ha='right', va='top', bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(min(x_values), min(y_values), 'Stable but Inaccurate', \n",
    "             fontsize=10, ha='left', va='bottom', bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(max(x_values), min(y_values), 'Problematic: Unstable & Inaccurate', \n",
    "             fontsize=10, ha='right', va='bottom', bbox=dict(facecolor='white', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "\n",
    "# Plot stability vs. performance for both models\n",
    "if resnet_metrics:\n",
    "    plt_resnet_scatter = plot_perf_vs_stability(resnet_metrics, \"ResNet-18\")\n",
    "    plt_resnet_scatter.show()\n",
    "\n",
    "if distilbert_metrics:\n",
    "    plt_distilbert_scatter = plot_perf_vs_stability(distilbert_metrics, \"DistilBERT\")\n",
    "    plt_distilbert_scatter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization Method Profiles\n",
    "\n",
    "Create a comprehensive profile of each initialization method across different metrics and architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_initialization_profiles(resnet_metrics, distilbert_metrics, init_methods):\n",
    "    \"\"\"Create DataFrame with profiles for each initialization method.\"\"\"\n",
    "    profiles = []\n",
    "    \n",
    "    for method in init_methods:\n",
    "        profile = {'Method': method.capitalize()}\n",
    "        \n",
    "        # Add ResNet metrics if available\n",
    "        if method in resnet_metrics:\n",
    "            metrics = resnet_metrics[method]\n",
    "            if 'final_accuracy' in metrics:\n",
    "                profile['ResNet Accuracy'] = f\"{metrics['final_accuracy']:.2f}%\"\n",
    "            if 'convergence_epoch' in metrics:\n",
    "                profile['ResNet Convergence'] = f\"{metrics['convergence_epoch']} epochs\"\n",
    "            if 'grad_norms' in metrics:\n",
    "
